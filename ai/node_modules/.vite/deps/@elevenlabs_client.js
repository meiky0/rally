// node_modules/@elevenlabs/client/dist/lib.modern.js
function e() {
  return e = Object.assign ? Object.assign.bind() : function(e2) {
    for (var t2 = 1; t2 < arguments.length; t2++) {
      var n2 = arguments[t2];
      for (var s2 in n2) ({}).hasOwnProperty.call(n2, s2) && (e2[s2] = n2[s2]);
    }
    return e2;
  }, e.apply(null, arguments);
}
var t = new Uint8Array(0);
var n = class {
  static getFullOptions(t2) {
    return e({ clientTools: {}, onConnect: () => {
    }, onDebug: () => {
    }, onDisconnect: () => {
    }, onError: () => {
    }, onMessage: () => {
    }, onAudio: () => {
    }, onModeChange: () => {
    }, onStatusChange: () => {
    }, onCanSendFeedbackChange: () => {
    } }, t2);
  }
  constructor(e2, t2) {
    var n2 = this;
    this.options = void 0, this.connection = void 0, this.lastInterruptTimestamp = 0, this.mode = "listening", this.status = "connecting", this.volume = 1, this.currentEventId = 1, this.lastFeedbackEventId = 1, this.canSendFeedback = false, this.endSessionWithDetails = async function(e3) {
      "connected" !== n2.status && "connecting" !== n2.status || (n2.updateStatus("disconnecting"), await n2.handleEndSession(), n2.updateStatus("disconnected"), n2.options.onDisconnect(e3));
    }, this.onMessage = async function(e3) {
      switch (e3.type) {
        case "interruption":
          return void n2.handleInterruption(e3);
        case "agent_response":
          return void n2.handleAgentResponse(e3);
        case "user_transcript":
          return void n2.handleUserTranscript(e3);
        case "internal_tentative_agent_response":
          return void n2.handleTentativeAgentResponse(e3);
        case "client_tool_call":
          return void await n2.handleClientToolCall(e3);
        case "audio":
          return void n2.handleAudio(e3);
        case "ping":
          return void n2.connection.sendMessage({ type: "pong", event_id: e3.ping_event.event_id });
        default:
          return void n2.options.onDebug(e3);
      }
    }, this.setVolume = ({ volume: e3 }) => {
      this.volume = e3;
    }, this.options = e2, this.connection = t2, this.options.onConnect({ conversationId: t2.conversationId }), this.connection.onMessage(this.onMessage), this.connection.onDisconnect(this.endSessionWithDetails), this.updateStatus("connected");
  }
  endSession() {
    return this.endSessionWithDetails({ reason: "user" });
  }
  async handleEndSession() {
    this.connection.close();
  }
  updateMode(e2) {
    e2 !== this.mode && (this.mode = e2, this.options.onModeChange({ mode: e2 }));
  }
  updateStatus(e2) {
    e2 !== this.status && (this.status = e2, this.options.onStatusChange({ status: e2 }));
  }
  updateCanSendFeedback() {
    const e2 = this.currentEventId !== this.lastFeedbackEventId;
    this.canSendFeedback !== e2 && (this.canSendFeedback = e2, this.options.onCanSendFeedbackChange({ canSendFeedback: e2 }));
  }
  handleInterruption(e2) {
    e2.interruption_event && (this.lastInterruptTimestamp = e2.interruption_event.event_id);
  }
  handleAgentResponse(e2) {
    this.options.onMessage({ source: "ai", message: e2.agent_response_event.agent_response });
  }
  handleUserTranscript(e2) {
    this.options.onMessage({ source: "user", message: e2.user_transcription_event.user_transcript });
  }
  handleTentativeAgentResponse(e2) {
    this.options.onDebug({ type: "tentative_agent_response", response: e2.tentative_agent_response_internal_event.tentative_agent_response });
  }
  async handleClientToolCall(e2) {
    if (this.options.clientTools.hasOwnProperty(e2.client_tool_call.tool_name)) try {
      var t2;
      const n2 = null != (t2 = await this.options.clientTools[e2.client_tool_call.tool_name](e2.client_tool_call.parameters)) ? t2 : "Client tool execution successful.", s2 = "object" == typeof n2 ? JSON.stringify(n2) : String(n2);
      this.connection.sendMessage({ type: "client_tool_result", tool_call_id: e2.client_tool_call.tool_call_id, result: s2, is_error: false });
    } catch (t3) {
      this.onError("Client tool execution failed with following error: " + (null == t3 ? void 0 : t3.message), { clientToolName: e2.client_tool_call.tool_name }), this.connection.sendMessage({ type: "client_tool_result", tool_call_id: e2.client_tool_call.tool_call_id, result: "Client tool execution failed: " + (null == t3 ? void 0 : t3.message), is_error: true });
    }
    else {
      if (this.options.onUnhandledClientToolCall) return void this.options.onUnhandledClientToolCall(e2.client_tool_call);
      this.onError(`Client tool with name ${e2.client_tool_call.tool_name} is not defined on client`, { clientToolName: e2.client_tool_call.tool_name }), this.connection.sendMessage({ type: "client_tool_result", tool_call_id: e2.client_tool_call.tool_call_id, result: `Client tool with name ${e2.client_tool_call.tool_name} is not defined on client`, is_error: true });
    }
  }
  handleAudio(e2) {
  }
  onError(e2, t2) {
    console.error(e2, t2), this.options.onError(e2, t2);
  }
  getId() {
    return this.connection.conversationId;
  }
  isOpen() {
    return "connected" === this.status;
  }
  setMicMuted(e2) {
  }
  getInputByteFrequencyData() {
    return t;
  }
  getOutputByteFrequencyData() {
    return t;
  }
  getInputVolume() {
    return 0;
  }
  getOutputVolume() {
    return 0;
  }
  sendFeedback(e2) {
    this.canSendFeedback ? (this.connection.sendMessage({ type: "feedback", score: e2 ? "like" : "dislike", event_id: this.currentEventId }), this.lastFeedbackEventId = this.currentEventId, this.updateCanSendFeedback()) : console.warn(0 === this.lastFeedbackEventId ? "Cannot send feedback: the conversation has not started yet." : "Cannot send feedback: feedback has already been sent for the current response.");
  }
  sendContextualUpdate(e2) {
    this.connection.sendMessage({ type: "contextual_update", text: e2 });
  }
  sendUserMessage(e2) {
    this.connection.sendMessage({ type: "user_message", text: e2 });
  }
  sendUserActivity() {
    this.connection.sendMessage({ type: "user_activity" });
  }
};
function s(e2) {
  return !!e2.type;
}
var a = class _a {
  static async create(e2) {
    let t2 = null;
    try {
      var n2;
      const i3 = null != (n2 = e2.origin) ? n2 : "wss://api.elevenlabs.io", r2 = e2.signedUrl ? e2.signedUrl : i3 + "/v1/convai/conversation?agent_id=" + e2.agentId, l2 = ["convai"];
      e2.authorization && l2.push(`bearer.${e2.authorization}`), t2 = new WebSocket(r2, l2);
      const c2 = await new Promise((n3, a2) => {
        t2.addEventListener("open", () => {
          var n4;
          const s2 = { type: "conversation_initiation_client_data" };
          var a3, o2, i4, r3, l3;
          e2.overrides && (s2.conversation_config_override = { agent: { prompt: null == (a3 = e2.overrides.agent) ? void 0 : a3.prompt, first_message: null == (o2 = e2.overrides.agent) ? void 0 : o2.firstMessage, language: null == (i4 = e2.overrides.agent) ? void 0 : i4.language }, tts: { voice_id: null == (r3 = e2.overrides.tts) ? void 0 : r3.voiceId }, conversation: { text_only: null == (l3 = e2.overrides.conversation) ? void 0 : l3.textOnly } }), e2.customLlmExtraBody && (s2.custom_llm_extra_body = e2.customLlmExtraBody), e2.dynamicVariables && (s2.dynamic_variables = e2.dynamicVariables), null == (n4 = t2) || n4.send(JSON.stringify(s2));
        }, { once: true }), t2.addEventListener("error", (e3) => {
          setTimeout(() => a2(e3), 0);
        }), t2.addEventListener("close", a2), t2.addEventListener("message", (e3) => {
          const t3 = JSON.parse(e3.data);
          s(t3) && ("conversation_initiation_metadata" === t3.type ? n3(t3.conversation_initiation_metadata_event) : console.warn("First received message is not conversation metadata."));
        }, { once: true });
      }), { conversation_id: u2, agent_output_audio_format: d2, user_input_audio_format: h2 } = c2, p2 = o(null != h2 ? h2 : "pcm_16000"), m2 = o(d2);
      return new _a(t2, u2, p2, m2);
    } catch (e3) {
      var i2;
      throw null == (i2 = t2) || i2.close(), e3;
    }
  }
  constructor(e2, t2, n2, a2) {
    this.socket = void 0, this.conversationId = void 0, this.inputFormat = void 0, this.outputFormat = void 0, this.queue = [], this.disconnectionDetails = null, this.onDisconnectCallback = null, this.onMessageCallback = null, this.socket = e2, this.conversationId = t2, this.inputFormat = n2, this.outputFormat = a2, this.socket.addEventListener("error", (e3) => {
      setTimeout(() => this.disconnect({ reason: "error", message: "The connection was closed due to a socket error.", context: e3 }), 0);
    }), this.socket.addEventListener("close", (e3) => {
      this.disconnect(1e3 === e3.code ? { reason: "agent", context: e3 } : { reason: "error", message: e3.reason || "The connection was closed by the server.", context: e3 });
    }), this.socket.addEventListener("message", (e3) => {
      try {
        const t3 = JSON.parse(e3.data);
        if (!s(t3)) return;
        this.onMessageCallback ? this.onMessageCallback(t3) : this.queue.push(t3);
      } catch (e4) {
      }
    });
  }
  close() {
    this.socket.close();
  }
  sendMessage(e2) {
    this.socket.send(JSON.stringify(e2));
  }
  onMessage(e2) {
    this.onMessageCallback = e2;
    const t2 = this.queue;
    this.queue = [], t2.length > 0 && queueMicrotask(() => {
      t2.forEach(e2);
    });
  }
  onDisconnect(e2) {
    this.onDisconnectCallback = e2;
    const t2 = this.disconnectionDetails;
    t2 && queueMicrotask(() => {
      e2(t2);
    });
  }
  disconnect(e2) {
    var t2;
    this.disconnectionDetails || (this.disconnectionDetails = e2, null == (t2 = this.onDisconnectCallback) || t2.call(this, e2));
  }
};
function o(e2) {
  const [t2, n2] = e2.split("_");
  if (!["pcm", "ulaw"].includes(t2)) throw new Error(`Invalid format: ${e2}`);
  const s2 = parseInt(n2);
  if (isNaN(s2)) throw new Error(`Invalid sample rate: ${n2}`);
  return { format: t2, sampleRate: s2 };
}
function i() {
  return ["iPad Simulator", "iPhone Simulator", "iPod Simulator", "iPad", "iPhone", "iPod"].includes(navigator.platform) || navigator.userAgent.includes("Mac") && "ontouchend" in document;
}
async function r(e2 = { default: 0, android: 3e3 }) {
  let t2 = e2.default;
  var n2;
  if (/android/i.test(navigator.userAgent)) t2 = null != (n2 = e2.android) ? n2 : t2;
  else if (i()) {
    var s2;
    t2 = null != (s2 = e2.ios) ? s2 : t2;
  }
  t2 > 0 && await new Promise((e3) => setTimeout(e3, t2));
}
var l = class _l extends n {
  static async startSession(e2) {
    const t2 = n.getFullOptions(e2);
    t2.onStatusChange({ status: "connecting" }), t2.onCanSendFeedbackChange({ canSendFeedback: false });
    let s2 = null;
    try {
      return await r(t2.connectionDelay), s2 = await a.create(e2), new _l(t2, s2);
    } catch (e3) {
      var o2;
      throw t2.onStatusChange({ status: "disconnected" }), null == (o2 = s2) || o2.close(), e3;
    }
  }
};
function c(e2) {
  const t2 = new Uint8Array(e2);
  return window.btoa(String.fromCharCode(...t2));
}
function u(e2) {
  const t2 = window.atob(e2), n2 = t2.length, s2 = new Uint8Array(n2);
  for (let e3 = 0; e3 < n2; e3++) s2[e3] = t2.charCodeAt(e3);
  return s2.buffer;
}
var d = /* @__PURE__ */ new Map();
function h(e2, t2) {
  return async (n2) => {
    const s2 = d.get(e2);
    if (s2) return n2.addModule(s2);
    const a2 = new Blob([t2], { type: "application/javascript" }), o2 = URL.createObjectURL(a2);
    try {
      return await n2.addModule(o2), void d.set(e2, o2);
    } catch (e3) {
      URL.revokeObjectURL(o2);
    }
    try {
      const s3 = `data:application/javascript;base64,${btoa(t2)}`;
      await n2.addModule(s3), d.set(e2, s3);
    } catch (t3) {
      throw new Error(`Failed to load the ${e2} worklet module. Make sure the browser supports AudioWorklets.`);
    }
  };
}
var p = h("raw-audio-processor", `
const BIAS = 0x84;
const CLIP = 32635;
const encodeTable = [
  0,0,1,1,2,2,2,2,3,3,3,3,3,3,3,3,
  4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,
  5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,
  5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,
  6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,
  6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,
  6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,
  6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,
  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,
  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,
  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,
  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,
  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,
  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,
  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,
  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7
];

function encodeSample(sample) {
  let sign;
  let exponent;
  let mantissa;
  let muLawSample;
  sign = (sample >> 8) & 0x80;
  if (sign !== 0) sample = -sample;
  sample = sample + BIAS;
  if (sample > CLIP) sample = CLIP;
  exponent = encodeTable[(sample>>7) & 0xFF];
  mantissa = (sample >> (exponent+3)) & 0x0F;
  muLawSample = ~(sign | (exponent << 4) | mantissa);
  
  return muLawSample;
}

class RawAudioProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
              
    this.port.onmessage = ({ data }) => {
      switch (data.type) {
        case "setFormat":
          this.isMuted = false;
          this.buffer = []; // Initialize an empty buffer
          this.bufferSize = data.sampleRate / 4;
          this.format = data.format;

          if (globalThis.LibSampleRate && sampleRate !== data.sampleRate) {
            globalThis.LibSampleRate.create(1, sampleRate, data.sampleRate).then(resampler => {
              this.resampler = resampler;
            });
          }
          break;
        case "setMuted":
          this.isMuted = data.isMuted;
          break;
      }
    };
  }
  process(inputs) {
    if (!this.buffer) {
      return true;
    }
    
    const input = inputs[0]; // Get the first input node
    if (input.length > 0) {
      let channelData = input[0]; // Get the first channel's data

      // Resample the audio if necessary
      if (this.resampler) {
        channelData = this.resampler.full(channelData);
      }

      // Add channel data to the buffer
      this.buffer.push(...channelData);
      // Get max volume 
      let sum = 0.0;
      for (let i = 0; i < channelData.length; i++) {
        sum += channelData[i] * channelData[i];
      }
      const maxVolume = Math.sqrt(sum / channelData.length);
      // Check if buffer size has reached or exceeded the threshold
      if (this.buffer.length >= this.bufferSize) {
        const float32Array = this.isMuted 
          ? new Float32Array(this.buffer.length)
          : new Float32Array(this.buffer);

        let encodedArray = this.format === "ulaw"
          ? new Uint8Array(float32Array.length)
          : new Int16Array(float32Array.length);

        // Iterate through the Float32Array and convert each sample to PCM16
        for (let i = 0; i < float32Array.length; i++) {
          // Clamp the value to the range [-1, 1]
          let sample = Math.max(-1, Math.min(1, float32Array[i]));

          // Scale the sample to the range [-32768, 32767]
          let value = sample < 0 ? sample * 32768 : sample * 32767;
          if (this.format === "ulaw") {
            value = encodeSample(Math.round(value));
          }

          encodedArray[i] = value;
        }

        // Send the buffered data to the main script
        this.port.postMessage([encodedArray, maxVolume]);

        // Clear the buffer after sending
        this.buffer = [];
      }
    }
    return true; // Continue processing
  }
}
registerProcessor("raw-audio-processor", RawAudioProcessor);
`);
var m = class _m {
  static async create({ sampleRate: e2, format: t2, preferHeadphonesForIosDevices: n2 }) {
    let s2 = null, a2 = null;
    try {
      const o3 = { sampleRate: { ideal: e2 }, echoCancellation: { ideal: true }, noiseSuppression: { ideal: true } };
      if (i() && n2) {
        const e3 = (await window.navigator.mediaDevices.enumerateDevices()).find((e4) => "audioinput" === e4.kind && ["airpod", "headphone", "earphone"].find((t3) => e4.label.toLowerCase().includes(t3)));
        e3 && (o3.deviceId = { ideal: e3.deviceId });
      }
      const r3 = navigator.mediaDevices.getSupportedConstraints().sampleRate;
      s2 = new window.AudioContext(r3 ? { sampleRate: e2 } : {});
      const l2 = s2.createAnalyser();
      r3 || await s2.audioWorklet.addModule("https://cdn.jsdelivr.net/npm/@alexanderolsen/libsamplerate-js@2.1.2/dist/libsamplerate.worklet.js"), await p(s2.audioWorklet), a2 = await navigator.mediaDevices.getUserMedia({ audio: o3 });
      const c2 = s2.createMediaStreamSource(a2), u2 = new AudioWorkletNode(s2, "raw-audio-processor");
      return u2.port.postMessage({ type: "setFormat", format: t2, sampleRate: e2 }), c2.connect(l2), l2.connect(u2), await s2.resume(), new _m(s2, l2, u2, a2);
    } catch (e3) {
      var o2, r2;
      throw null == (o2 = a2) || o2.getTracks().forEach((e4) => e4.stop()), null == (r2 = s2) || r2.close(), e3;
    }
  }
  constructor(e2, t2, n2, s2) {
    this.context = void 0, this.analyser = void 0, this.worklet = void 0, this.inputStream = void 0, this.context = e2, this.analyser = t2, this.worklet = n2, this.inputStream = s2;
  }
  async close() {
    this.inputStream.getTracks().forEach((e2) => e2.stop()), await this.context.close();
  }
  setMuted(e2) {
    this.worklet.port.postMessage({ type: "setMuted", isMuted: e2 });
  }
};
var f = h("audio-concat-processor", '\nconst decodeTable = [0,132,396,924,1980,4092,8316,16764];\n\nexport function decodeSample(muLawSample) {\n  let sign;\n  let exponent;\n  let mantissa;\n  let sample;\n  muLawSample = ~muLawSample;\n  sign = (muLawSample & 0x80);\n  exponent = (muLawSample >> 4) & 0x07;\n  mantissa = muLawSample & 0x0F;\n  sample = decodeTable[exponent] + (mantissa << (exponent+3));\n  if (sign !== 0) sample = -sample;\n\n  return sample;\n}\n\nclass AudioConcatProcessor extends AudioWorkletProcessor {\n  constructor() {\n    super();\n    this.buffers = []; // Initialize an empty buffer\n    this.cursor = 0;\n    this.currentBuffer = null;\n    this.wasInterrupted = false;\n    this.finished = false;\n    \n    this.port.onmessage = ({ data }) => {\n      switch (data.type) {\n        case "setFormat":\n          this.format = data.format;\n          break;\n        case "buffer":\n          this.wasInterrupted = false;\n          this.buffers.push(\n            this.format === "ulaw"\n              ? new Uint8Array(data.buffer)\n              : new Int16Array(data.buffer)\n          );\n          break;\n        case "interrupt":\n          this.wasInterrupted = true;\n          break;\n        case "clearInterrupted":\n          if (this.wasInterrupted) {\n            this.wasInterrupted = false;\n            this.buffers = [];\n            this.currentBuffer = null;\n          }\n      }\n    };\n  }\n  process(_, outputs) {\n    let finished = false;\n    const output = outputs[0][0];\n    for (let i = 0; i < output.length; i++) {\n      if (!this.currentBuffer) {\n        if (this.buffers.length === 0) {\n          finished = true;\n          break;\n        }\n        this.currentBuffer = this.buffers.shift();\n        this.cursor = 0;\n      }\n\n      let value = this.currentBuffer[this.cursor];\n      if (this.format === "ulaw") {\n        value = decodeSample(value);\n      }\n      output[i] = value / 32768;\n      this.cursor++;\n\n      if (this.cursor >= this.currentBuffer.length) {\n        this.currentBuffer = null;\n      }\n    }\n\n    if (this.finished !== finished) {\n      this.finished = finished;\n      this.port.postMessage({ type: "process", finished });\n    }\n\n    return true; // Continue processing\n  }\n}\n\nregisterProcessor("audio-concat-processor", AudioConcatProcessor);\n');
var g = class _g {
  static async create({ sampleRate: e2, format: t2 }) {
    let n2 = null;
    try {
      n2 = new AudioContext({ sampleRate: e2 });
      const s3 = n2.createAnalyser(), a2 = n2.createGain();
      a2.connect(s3), s3.connect(n2.destination), await f(n2.audioWorklet);
      const o2 = new AudioWorkletNode(n2, "audio-concat-processor");
      return o2.port.postMessage({ type: "setFormat", format: t2 }), o2.connect(a2), await n2.resume(), new _g(n2, s3, a2, o2);
    } catch (e3) {
      var s2;
      throw null == (s2 = n2) || s2.close(), e3;
    }
  }
  constructor(e2, t2, n2, s2) {
    this.context = void 0, this.analyser = void 0, this.gain = void 0, this.worklet = void 0, this.context = e2, this.analyser = t2, this.gain = n2, this.worklet = s2;
  }
  async close() {
    await this.context.close();
  }
};
var v = class _v extends n {
  static async startSession(t2) {
    var s2;
    const o2 = n.getFullOptions(t2);
    o2.onStatusChange({ status: "connecting" }), o2.onCanSendFeedbackChange({ canSendFeedback: false });
    let i2 = null, l2 = null, c2 = null, u2 = null, d2 = null;
    if (null == (s2 = t2.useWakeLock) || s2) try {
      d2 = await navigator.wakeLock.request("screen");
    } catch (e2) {
    }
    try {
      var h2;
      return u2 = await navigator.mediaDevices.getUserMedia({ audio: true }), await r(o2.connectionDelay), l2 = await a.create(t2), [i2, c2] = await Promise.all([m.create(e({}, l2.inputFormat, { preferHeadphonesForIosDevices: t2.preferHeadphonesForIosDevices })), g.create(l2.outputFormat)]), null == (h2 = u2) || h2.getTracks().forEach((e2) => e2.stop()), u2 = null, new _v(o2, l2, i2, c2, d2);
    } catch (e2) {
      var p2, f2, y2, w2;
      o2.onStatusChange({ status: "disconnected" }), null == (p2 = u2) || p2.getTracks().forEach((e3) => e3.stop()), null == (f2 = l2) || f2.close(), await (null == (y2 = i2) ? void 0 : y2.close()), await (null == (w2 = c2) ? void 0 : w2.close());
      try {
        var _;
        await (null == (_ = d2) ? void 0 : _.release()), d2 = null;
      } catch (e3) {
      }
      throw e2;
    }
  }
  constructor(e2, t2, n2, s2, a2) {
    super(e2, t2), this.input = void 0, this.output = void 0, this.wakeLock = void 0, this.inputFrequencyData = void 0, this.outputFrequencyData = void 0, this.onInputWorkletMessage = (e3) => {
      "connected" === this.status && this.connection.sendMessage({ user_audio_chunk: c(e3.data[0].buffer) });
    }, this.onOutputWorkletMessage = ({ data: e3 }) => {
      "process" === e3.type && this.updateMode(e3.finished ? "listening" : "speaking");
    }, this.addAudioBase64Chunk = (e3) => {
      this.output.gain.gain.value = this.volume, this.output.worklet.port.postMessage({ type: "clearInterrupted" }), this.output.worklet.port.postMessage({ type: "buffer", buffer: u(e3) });
    }, this.fadeOutAudio = () => {
      this.updateMode("listening"), this.output.worklet.port.postMessage({ type: "interrupt" }), this.output.gain.gain.exponentialRampToValueAtTime(1e-4, this.output.context.currentTime + 2), setTimeout(() => {
        this.output.gain.gain.value = this.volume, this.output.worklet.port.postMessage({ type: "clearInterrupted" });
      }, 2e3);
    }, this.calculateVolume = (e3) => {
      if (0 === e3.length) return 0;
      let t3 = 0;
      for (let n3 = 0; n3 < e3.length; n3++) t3 += e3[n3] / 255;
      return t3 /= e3.length, t3 < 0 ? 0 : t3 > 1 ? 1 : t3;
    }, this.input = n2, this.output = s2, this.wakeLock = a2, this.input.worklet.port.onmessage = this.onInputWorkletMessage, this.output.worklet.port.onmessage = this.onOutputWorkletMessage;
  }
  async handleEndSession() {
    await super.handleEndSession();
    try {
      var e2;
      await (null == (e2 = this.wakeLock) ? void 0 : e2.release()), this.wakeLock = null;
    } catch (e3) {
    }
    await this.input.close(), await this.output.close();
  }
  handleInterruption(e2) {
    super.handleInterruption(e2), this.fadeOutAudio();
  }
  handleAudio(e2) {
    this.lastInterruptTimestamp <= e2.audio_event.event_id && (this.options.onAudio(e2.audio_event.audio_base_64), this.addAudioBase64Chunk(e2.audio_event.audio_base_64), this.currentEventId = e2.audio_event.event_id, this.updateCanSendFeedback(), this.updateMode("speaking"));
  }
  setMicMuted(e2) {
    this.input.setMuted(e2);
  }
  getInputByteFrequencyData() {
    return null != this.inputFrequencyData || (this.inputFrequencyData = new Uint8Array(this.input.analyser.frequencyBinCount)), this.input.analyser.getByteFrequencyData(this.inputFrequencyData), this.inputFrequencyData;
  }
  getOutputByteFrequencyData() {
    return null != this.outputFrequencyData || (this.outputFrequencyData = new Uint8Array(this.output.analyser.frequencyBinCount)), this.output.analyser.getByteFrequencyData(this.outputFrequencyData), this.outputFrequencyData;
  }
  getInputVolume() {
    return this.calculateVolume(this.getInputByteFrequencyData());
  }
  getOutputVolume() {
    return this.calculateVolume(this.getOutputByteFrequencyData());
  }
};
function y(e2, t2, n2 = "https://api.elevenlabs.io") {
  return fetch(`${n2}/v1/convai/conversations/${e2}/feedback`, { method: "POST", body: JSON.stringify({ feedback: t2 ? "like" : "dislike" }), headers: { "Content-Type": "application/json" } });
}
var w = class extends n {
  static startSession(e2) {
    return e2.textOnly ? l.startSession(e2) : v.startSession(e2);
  }
};
export {
  w as Conversation,
  y as postOverallFeedback
};
//# sourceMappingURL=@elevenlabs_client.js.map
